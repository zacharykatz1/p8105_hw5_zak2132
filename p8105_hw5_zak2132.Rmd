---
title: 'P8105 Homework #5'
author: 'Zachary Katz (UNI: zak2132)'
date: "11/20/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(tidyverse)
library(viridis)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r import, message = FALSE}
# Import CSV and clean names
homicide_df = read_csv(file = "./Data/homicide-data.csv", na = c("", "Unknown")) %>% 
  janitor::clean_names()

# Obtain head of raw data
head(homicide_df)

# Obtain structure of raw data
str(homicide_df)
```

`homicide_df` is a collection of `r nrow(homicide_df)` observations on `r ncol(homicide_df)` variables from The Washington Post. Each observation represents a criminal homicide (i.e. unique victim). Key variables for each homicide include `reported_date` (reported date of homicide), demographic variables for each victim including first and last name (`victim_first` and `victim_last`, respectively), `victim_race`, `victim_age`, and `victim_sex`, as well as location of homicide in both geographical name (`city` and `state`) and coordinates (`lat` and `long`). `disposition` informs us whether a case remains open or closed, including if an arrest was or wasn't made. Data comes from `r n_distinct(pull(homicide_df, state))` states and covers the period from `r min(pull(homicide_df, reported_date))` to `r max(pull(homicide_df, reported_date))`.

We can also summarize the raw data as follows, finding that 60 homicide observations are missing latitude and longitude coordinates and the 28-state data represents 50 unique cities.

```{r}
summary(homicide_df)

skimr::skim(homicide_df)
```

Let's clean the data a bit, including using appropriate variable types.

```{r clean data, message = FALSE, warning = FALSE}
# Clean the data
homicide_df = homicide_df %>% 
  # Change data types where warranted
  # Not changing date using `lubridate` because no HW analysis requires date var
  mutate(
    victim_sex = as.factor(victim_sex),
    state = as.factor(state)
  ) %>% 
  # Create `city_state` variable and remove unnecessary variables
  mutate(
    city_state = 
      str_c(city, ", ", state),
    # New column for solved or unsolved comes in handy later
    solved_or_not = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  ) %>% 
  select(-city, -state, -disposition) %>% 
  # Filter out likely error in data entry
  filter(city_state != "Tulsa, AL")
```

We want to summarize within cities to obtain the total number of homicides, and the number of unsolved homicides (with disposition "Closed without arrest" or "Open/No arrest"). We can do this as follows:

```{r count homicides and unsolved homicides by city}
# Total number of homicides by city
homicide_df_cities = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(solved_or_not == "unsolved"),
    percent_unsolved = round(unsolved_homicides*100 / total_homicides, 1)
  ) %>% 
  arrange(desc(total_homicides))

homicide_df_cities %>% 
  knitr::kable(
    col.names = c("City/State", "Total Homicides", "Unsolved Homicides", "% Unsolved")
  )
```

Chicago has the most homicides, while Tulsa (AL) has the fewest.

We'd like to obtain the estimated proportion of homicides that are unsolved, first for Baltimore, and then for each city, along with confidence intervals for these estimates.

```{r baltimore only}
# Perform proportion test on Baltimore and save as object
prop_test_baltimore = prop.test(
  homicide_df_cities %>% filter(city_state == "Baltimore, MD") %>% pull(unsolved_homicides),
  homicide_df_cities %>% filter(city_state == "Baltimore, MD") %>% 
    pull(total_homicides)
)

# Apply broom::tidy and pull estimated proportion and confidence intervals
tidy_balt_conf = 
  broom::tidy(prop_test_baltimore) %>% 
  select(estimate, conf.low, conf.high)

# Put into nice table
tidy_balt_conf %>% 
  knitr::kable()
```

Now that we've done it for Baltimore, let's iterate over all cities:

```{r iterate prop test, warning = FALSE}
# Iterate over cities
tests = homicide_df_cities %>% 
  # Apply prop_tests to each row
  mutate(
    prop_tests = map2(.x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    # Tidy proportion test results over cities
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  arrange(desc(estimate))

tests %>% 
  knitr::kable(
    col.names = c("City/State", "Estimated Proportion Unsolved", "CI Lower Bound", "CI Upper Bound")
  )
```

Chicago, IL also has the highest estimated proportion of unsolved murders. New Orleans, LA and Baltimore, MD aren't great either!

Finally, let's create a plot that shows the estimates and CIs for each city, organizing the cities according to the proportion of unsolved homicides.

```{r plot unsolved estimates}
tests %>% 
  # Generate percentages from proportions
  mutate(
    estimate = 100*estimate,
    conf.low = 100*conf.low,
    conf.high = 100*conf.high
  ) %>% 
  # Reorder cities by estimated proportion unsolved
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(
    axis.text.x = element_text(angle = 60, vjust = 1.0, hjust = 1)
  ) + 
  labs(
    title = "Estimated Proportion Unsolved Homicides by City",
    x = "City",
    y = "% Estimated Unsolved"
  )
```

## Problem 2

We need to read in data from individual participants in our longitudinal study, with each participant's data stored in a separate file.

```{r import and tidy problem 2 data, message = FALSE, warning = FALSE}

study_df = tibble(
  # Find list of paths to determine commonalities
  path = list.files("./Data/problem_2_data")
) %>% 
  # Iterate over paths to import data, then bind together
  mutate(
    participant_id = str_remove(path, ".csv"),
    path = str_c("./Data/problem_2_data/", path),
    data = map(path, read_csv),
    data = map(data, bind_rows)
  ) %>% 
  # Glean information from file path name into each row
  separate(participant_id, into = c("arm", "subject_id", sep = "_")) %>% 
  # Rename and factorize variable as needed
  mutate(
    arm = as.factor(
      recode(
        arm,
        "con" = "Control",
        "exp" = "Experimental")
    )
  ) %>% 
  select(-4, -path) %>%
  unnest(data) %>% 
  # Reorder columns to be more suitable (identifiers up front)
  select(subject_id, arm, everything()) %>% 
  # Tidy data by ensuring each row represents an observation
  # Each observation is a study participant's observed data point in a given week
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "data_point"
  ) %>% 
  # Make week numeric
  mutate(
    week = as.numeric(week)
  ) 

study_df %>% 
  head() %>% 
  knitr::kable()
```

Our data looks nice and tidy! Let's take a quick look at the structure and summary:

```{r problem 2 structure and summary}
str(study_df)

summary(study_df)
```

In total, there are `r nrow(study_df)` observations, where each observation represents a study participant's observed data value in a given week, and `r ncol(study_df)` variables, which include `subject_id` (unique identifier for each participant), `arm` (factor variable for control or experimental), `week` (week observation was made), and `data_point` (observed value in study).

We'd like to create a spaghetti plot showing observations on each subject over time:

```{r create spaghetti plot, warning = FALSE, message = FALSE}
study_df %>% 
  ggplot(
    aes(
      x = week,
      y = data_point,
      # Combine two variables into one factor
      group = interaction(arm, subject_id),
      # Color by treatment arm
      color = arm)
    ) + 
      # Increase transparency for individual lines in spaghetti plot
      geom_line(size = 1, alpha = 0.2) +
      # Add line of best fit for each study arm
      geom_smooth(aes(group = as.factor(arm)), alpha = 1.5, se = FALSE) + 
  labs(
    title = "Observed Data Value Over Time By Participant",
    subtitle = "Colored by Study Arm",
    x = "Week",
    y = "Observed Data Point",
    color = "Study Arm"
  )
```

In this graph, each translucent line represents one participant's recorded observation as it changes longitudinally, while the dark purple and yellow lines represent estimated conditional means over time for the control vs. experimental arms. Generally, it appears as though those in the experimental group see an increased in their observed data point by the end of eight weeks, whereas those in the control group see a mild decrease over the same study period.

## Problem 3

First, we want to load the `iris` dataset and introduce missing values at random in each column.

```{r introduce missing iris values}
# Set seed
set.seed(10)

# Iteratively replace with missing values
iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

# Clean var names
iris_with_missing = iris_with_missing %>% 
  janitor::clean_names()
```

For numeric variables, we want to fill in missing values with the mean of non-missing values, whereas for character variables, we want to fill in missing values with "virginica." Let's write a function that takes a vector as an argument, replaces missing values according to these rules, and returns the resulting vector. Then, we can apply it to the columns of `iris_with_missing` using a map statement.

```{r function to replace missing values, apply to iris_with_missing}
# Create function
replace_missing = function(x) {
  
  if (is.numeric(x)) {
    # Replace missing values with mean of non-missing values
    replace_na(x, mean(x, na.rm = TRUE))
  } else if (is.character(x)) {
    replace_na(x, "virginica")
  } else {
    stop("Cannot be computed for non-character, non-numeric variables")
  }
  
}

# Apply it to iris_with_missing, ensuring we return a data frame
iris_replaced_missing = map_df(.x = iris_with_missing, ~replace_missing(.x)) %>% 
  knitr::kable()

# Print result
iris_replaced_missing
```


